{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import json\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config\n",
    "with open('config.json') as json_file:\n",
    "    f = json.load(json_file)\n",
    "    NUMBER_OF_QUERIES = f['number_of_queries']\n",
    "    REPETITIONS = f['repetitions']\n",
    "    ROWS = f['rows']\n",
    "    SELECTIVITIES = [f['selectivity']]\n",
    "    COLS = f['cols']\n",
    "    EXPS_DEFAULTS = f['experiments']\n",
    "    PARTITION_SIZE = f['partition_size']\n",
    "    PROGRESSIVE_INDEX_DELTAS = f['deltas']\n",
    "\n",
    "# Algorithms and Experiments defitions\n",
    "algorithms = {\n",
    "    'AverageKDTree': {\n",
    "        'name': 'average_kd_tree',\n",
    "        'color': 'red',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'AvgKD',\n",
    "        'type': 'full_index',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'MedianKDTree': {\n",
    "        'name': 'median_kd_tree',\n",
    "        'color': 'red',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'MedKD',\n",
    "        'type': 'full_index',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'CrackingKDTree': {\n",
    "        'name': 'cracking_kd_tree',\n",
    "        'color': 'green',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'AKD',\n",
    "        'type': 'adaptive',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'Quasii': {\n",
    "        'name': 'quasii',\n",
    "        'color': 'green',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'Q',\n",
    "        'type': 'adaptive',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'FullScan': {\n",
    "        'name': 'full_scan_cl',\n",
    "        'color': 'black',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'FS',\n",
    "        'type': 'full_index',\n",
    "        'partition_size': '0',\n",
    "        'delta':'0.0',\n",
    "    },\n",
    "    'ProgressiveIndexCostModel': {\n",
    "        'name': 'progressive_index_cm',\n",
    "        'color': 'purple',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'GPKD(.2)',\n",
    "        'type': 'adaptive',\n",
    "        'delta': PROGRESSIVE_INDEX_DELTAS[0],\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'ProgressiveIndex': {\n",
    "        'name': 'progressive_index',\n",
    "        'color': 'purple',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'PKD(.2)',\n",
    "        'type': 'adaptive',\n",
    "        'delta': PROGRESSIVE_INDEX_DELTAS[0],\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, value in algorithms.items():\n",
    "    name = value['name']\n",
    "    delta = value['delta']\n",
    "    partition_size = value['partition_size']\n",
    "    algorithms[key]['alg_id'] = f\"{name}-{delta}-{partition_size}\"\n",
    "    \n",
    "# Synthetic Experiments\n",
    "experiments = {}\n",
    "\n",
    "for i in [2, 4, 8, 16]:\n",
    "    temp = {\n",
    "        f'Uniform{i}': {\n",
    "            \"name\": f\"Unif({i})\",\n",
    "            'name-in-file': 'uniform',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'Skewed{i}': {\n",
    "            \"name\": f\"Skewed({i})\",\n",
    "            'name-in-file': 'skewed',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'Sequential{i}': {\n",
    "            \"name\": f\"Seq ({i})\",\n",
    "            'name-in-file': 'sequential',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'Periodic{i}': {\n",
    "            \"name\": f\"Prdc({i})\",\n",
    "            'name-in-file': 'periodic',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'ZoomIn{i}': {\n",
    "            \"name\": f\"Zoom({i})\",\n",
    "            'name-in-file': 'zoom_in',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'SequentialZoomIn{i}': {\n",
    "            \"name\": f\"SeqZoom({i})\",\n",
    "            'name-in-file': 'sequential_zoom_in',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'AlternatingZoomIn{i}': {\n",
    "            \"name\": f\"AltZoom({i})\",\n",
    "            'name-in-file': 'alternating_zoom_in',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        },\n",
    "        f'Shifting{i}': {\n",
    "            \"name\": f\"Shift({i})\",\n",
    "            'name-in-file': '',\n",
    "            'n_rows': ROWS,\n",
    "            'n_queries': NUMBER_OF_QUERIES,\n",
    "            'n_cols': f'{i}',\n",
    "            'sel': '0.01',\n",
    "        }\n",
    "    }\n",
    "    experiments = {**experiments, **temp}\n",
    "    \n",
    "for key, value in experiments.items():\n",
    "    name = value['name-in-file']\n",
    "    rows = value['n_rows']\n",
    "    n_queries = value['n_queries']\n",
    "    cols = value['n_cols']\n",
    "    sel = value['sel']\n",
    "    experiments[key]['exp_id'] = f\"{name}-{rows}-{n_queries}-{cols}-{sel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output\n",
    "def read(alg, exp):\n",
    "    if exp.startswith('Shifting'):\n",
    "        n_queries_per_run = 10\n",
    "        uni = read(alg, 'Uniform' + experiments[exp]['n_cols'])\n",
    "        n_runs = int(len(uni)/n_queries_per_run) - 1\n",
    "        df_final = uni.head(n_queries_per_run)\n",
    "        for _ in range(int(n_runs)):\n",
    "            temp = uni.head(n_queries_per_run).copy()\n",
    "            df_final = df_final.append(temp, ignore_index=True)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"results/{algorithms[alg]['alg_id']}-{experiments[exp]['exp_id']}.csv\")\n",
    "        repetitions = df['repetition'].max() + 1\n",
    "        step = int(len(df.index)/repetitions)\n",
    "        df_final = df[:step].copy().reset_index()\n",
    "        for rep in range(1, repetitions):\n",
    "            df_final += df[step * (rep) : step * (rep + 1)].copy().reset_index()\n",
    "\n",
    "        df_final = df_final/repetitions\n",
    "    \n",
    "    if 'index_search_time' not in df_final:\n",
    "        df_final['index_search_time'] = 0.0\n",
    "    if 'tuples_scanned' not in df_final:\n",
    "        df_final['tuples_scanned'] = 0.0\n",
    "    if 'number_of_nodes' not in df_final:\n",
    "        df_final['number_of_nodes'] = 0.0\n",
    "    df_final['query_time'] = df_final['initialization_time'] + df_final['index_search_time'] + df_final['scan_time'] + df_final['adaptation_time']\n",
    "    df_final['query_time_cumsum'] = df_final['query_time'].cumsum()\n",
    "    return df_final\n",
    "\n",
    "                     \n",
    "def read_multiple(algs, exp):\n",
    "    ''' Reads multiple algorithms in an experiment, return three arrays: dfs, colors, names\n",
    "    '''\n",
    "    dfs = []\n",
    "    colors = []\n",
    "    names = []\n",
    "    dashes = []\n",
    "    for alg in algs:\n",
    "        dfs.append(read(alg, exp))\n",
    "        names.append(algorithms[alg]['show_name'])\n",
    "        colors.append(algorithms[alg]['color'])\n",
    "        dashes.append(algorithms[alg]['dash'])\n",
    "    \n",
    "    return dfs, colors, dashes, names,\n",
    "\n",
    "                     \n",
    "def save_figure(fig, fig_name):\n",
    "    fig.write_image(f\"figures/{fig_name}\", width=1024, height=768)\n",
    "                     \n",
    "def save_table(table, table_name):\n",
    "    with open(f\"tables/{table_name}\", 'w') as f:\n",
    "        f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def get_first_query(df):\n",
    "    return df['query_time'].iloc[0]\n",
    "\n",
    "def get_payoff(df, baseline):\n",
    "    p = [i for i, x in enumerate(df['query_time_cumsum'] - (baseline['query_time_cumsum'])) if x > 0]\n",
    "    if len(p) == 0:\n",
    "        return len(df)\n",
    "    return p[-1]\n",
    "\n",
    "def get_convergence(df, df_type=''):\n",
    "    if df_type == 'full_index':\n",
    "        return 0\n",
    "    c = [i for i, x in enumerate(df['adaptation_time']) if x != 0.0]\n",
    "    if(len(c) == 0):\n",
    "        return len(c)\n",
    "    else:\n",
    "        return c[-1]\n",
    "\n",
    "def get_robustness(df, df_type=''):\n",
    "    if df_type == 'full_index':\n",
    "        return 0\n",
    "    return np.var(df['query_time'][:min(50, get_convergence(df, df_type))])\n",
    "\n",
    "def get_total_time(df, lower=0, upper=-1):\n",
    "    return df['query_time'][lower:upper].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "delta_cols = [2, 4, 5, 6, 7, 8]\n",
    "delta_markers = ['circle', 'square', 'x', 'star', 'triangle-up', 'diamond']\n",
    "\n",
    "def create_figure(data=[]):\n",
    "    fig = go.Figure(\n",
    "        data=data,\n",
    "        layout=go.Layout(\n",
    "#             width=1500,\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            font=dict(\n",
    "                size=42\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                showline=True,\n",
    "                linewidth=2,\n",
    "                linecolor='black',\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor='lightgrey',\n",
    "                zeroline=False,\n",
    "                zerolinecolor='rgba(0, 0, 0, 0)',\n",
    "                ticks=\"inside\",\n",
    "                ticklen=5\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                showline=True,\n",
    "                linewidth=2,\n",
    "                linecolor='black',\n",
    "                ticks='inside',\n",
    "                zeroline=True,\n",
    "                ticklen=5\n",
    "            ),\n",
    "            legend=dict(\n",
    "                font=dict(\n",
    "                    size=30,\n",
    "                    color=\"black\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def line(exp, algs, attr, limit=2000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df[attr][:limit])\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "#                 marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=4, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(showlegend=True, yaxis_title=attr)\n",
    "    return fig\n",
    "\n",
    "def workload_selectivity(exp):\n",
    "    fig = create_figure()\n",
    "    df = read('FullScan', exp)\n",
    "    sel = ((df['tuples_scanned']/df['scan_overhead'])/df['tuples_scanned']) * 100\n",
    "    \n",
    "    fig.add_traces(\n",
    "        data=go.Scatter(\n",
    "            name='selectivity',\n",
    "            x=list(range(len(sel))),\n",
    "            y=sel,\n",
    "            mode='lines',\n",
    "            line=dict(width=4)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Selectivity (%)')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def per_query(exp, algs, limit=1000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        alg = algs[i]\n",
    "        per_query_times = np.array(df['query_time'][:limit]) * 1000\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Time (milliseconds)')\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_yaxes(type=\"log\")\n",
    "    return fig\n",
    "\n",
    "def cummulative(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        alg = algs[i]\n",
    "        per_query_times = np.array(df['query_time_cumsum'][:limit])\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Time (Seconds)')\n",
    "    return fig\n",
    "\n",
    "def number_of_nodes(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df['number_of_nodes'][:limit])\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='# Nodes')\n",
    "    return fig\n",
    "\n",
    "def tuples_scanned(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df['tuples_scanned'][:limit])\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Tuples Scanned')\n",
    "    return fig\n",
    "\n",
    "def break_down(exp, algs, limit):\n",
    "    dfs, _, _, names = read_multiple(algs, exp)\n",
    "    initializations = np.array([x['initialization_time'][:limit].sum() for x in dfs])\n",
    "    adaptation = np.array([x['adaptation_time'][:limit].sum() for x in dfs])\n",
    "    search = np.array([x['index_search_time'][:limit].sum() for x in dfs])\n",
    "    scan = np.array([x['scan_time'][:limit].sum() for x in dfs])\n",
    "    \n",
    "#     fig = create_figure(data=[\n",
    "#         go.Bar(name='Initialization', x=names, y=initializations, marker_color='rgb(176, 201, 146)'),\n",
    "#         go.Bar(name='Adaptation', x=names, y=adaptation, marker_color='rgb(195, 114, 28)'),\n",
    "#         go.Bar(name='Index Search', x=names, y=search, marker_color='rgb(197, 255, 114)'),\n",
    "#         go.Bar(name='Scan', x=names, y=scan, marker_color='rgb(237, 218, 123)'),\n",
    "#     ])\n",
    "\n",
    "    scan_color = '#AB63FA'\n",
    "    index_search_color = '#03CC96'\n",
    "    adaptation_color = '#EF553B'\n",
    "    initialization_color = '#636EFA'\n",
    "\n",
    "    data = [\n",
    "        go.Bar(name='Initialization', x=names, y=initializations, width=0.5, marker_color=initialization_color, showlegend=False),\n",
    "        go.Bar(name='Adaptation', x=names, y=adaptation, width=0.5, marker_color=adaptation_color, showlegend=False),\n",
    "        go.Bar(name='Index Search', x=names, y=search, width=0.5, marker_color=index_search_color,showlegend=False),\n",
    "        go.Bar(name='Scan', x=names, y=scan, width=0.5, marker_color=scan_color,showlegend=False),\n",
    "    ]\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Initialization',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=initialization_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Adaptation',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=adaptation_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Index Search',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=index_search_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Scan',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=scan_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = create_figure(data=data)\n",
    "    \n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(yaxis_title='Time (seconds)')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex Tables\n",
    "\n",
    "def pandas_to_latex(df, highlight='min', ignore_last=False, float_format=\"%.2f\"):\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        c_min = float('inf')\n",
    "        c_max = -float('inf')\n",
    "        if ignore_last:\n",
    "            length = len(row) - 1\n",
    "        else:\n",
    "            length = len(row)\n",
    "        for j in range(length):\n",
    "            if row[j] == '-' or row[j] == '*':\n",
    "                continue\n",
    "            if c_min > float(row[j]):\n",
    "                c_min = float(row[j])\n",
    "            if c_max < float(row[j]):\n",
    "                c_max = float(row[j])\n",
    "        mins.append(c_min)\n",
    "        maxs.append(c_max)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        for j in range(len(row)):\n",
    "            if row[j] == '-' or row[j] == '*':\n",
    "                continue\n",
    "            if highlight == 'min':\n",
    "                if float(row[j]) == mins[i]:\n",
    "                    row[j] = \"\\cellcolor{green!25}\" + (float_format % float(row[j]))\n",
    "                else:\n",
    "                    row[j] = float_format % float(row[j])\n",
    "            if highlight == 'max':\n",
    "                if float(row[j]) == maxs[i]:\n",
    "                    row[j] = \"\\cellcolor{green!25}\" + (float_format % float(row[j]))\n",
    "                else:\n",
    "                    row[j] = float_format % float(row[j])\n",
    "    \n",
    "    return df.to_latex(multicolumn=True, multirow=True, escape=False)\n",
    "\n",
    "def metrics(exps, algs):\n",
    "    '''\n",
    "    ||||||||||||Algorithm 1| Algorithm 2|...\n",
    "    First Query|   1.11.   |     15.    |...\n",
    "    ...\n",
    "    '''\n",
    "    data = {}\n",
    "\n",
    "    # create indexes\n",
    "    index_exp = []\n",
    "    index_metric = []\n",
    "    \n",
    "    \n",
    "    metrics = ['First Query', 'PayOff', 'Convergence', 'Robustness', 'Time']\n",
    "\n",
    "    \n",
    "    for exp in exps:\n",
    "        dfs, _, _, names = read_multiple(algs, exp)\n",
    "        \n",
    "        # initialize the data dict with empty arrays for each algorithm\n",
    "        for name in names:\n",
    "            if name not in data:\n",
    "                data[name] = []\n",
    "\n",
    "        baseline = read('FullScan', exp)\n",
    "\n",
    "        index_exp += [experiments[exp]['name']] * len(metrics)\n",
    "        index_metric += metrics\n",
    "        \n",
    "        for df, name, alg in zip(dfs, names, algs):\n",
    "            data[name].append('%.2f' % get_first_query(df))\n",
    "            po = get_payoff(df, baseline)\n",
    "            if po == len(df):\n",
    "                data[name].append('-')\n",
    "            else:\n",
    "                data[name].append(po)\n",
    "            conv = get_convergence(df, algorithms[alg]['type'])\n",
    "            if conv == 0:\n",
    "                data[name].append('-')\n",
    "            elif conv >= len(df)-1:\n",
    "                data[name].append('*')\n",
    "            else:\n",
    "                data[name].append(conv)\n",
    "            \n",
    "            robust = get_robustness(df, algorithms[alg]['type'])\n",
    "            if robust == 0:\n",
    "                data[name].append('-') \n",
    "            else:\n",
    "                data[name].append('%.E' % robust)\n",
    "            \n",
    "            \n",
    "            data[name].append('%.2f' %get_total_time(df))\n",
    "\n",
    "    index = [index_exp, index_metric]\n",
    "    df = pd.DataFrame(data, index=index)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    latex = df.to_latex(multicolumn=True, multirow=True)\n",
    "\n",
    "    for exp in exps:\n",
    "        latex = latex.replace(exp, \"\\\\rotatebox[origin=c]{90}{%s}\" % exp)\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'results/progressive_index-0.2-1024-zoom_in-70000000-1000-8-0.01.csv' does not exist: b'results/progressive_index-0.2-1024-zoom_in-70000000-1000-8-0.01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8f0226a7710f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'ProgressiveIndex'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m'ProgressiveIndexCostModel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m'FullScan'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     11\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-861910e596c4>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(exps, algs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# initialize the data dict with empty arrays for each algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-36b365ade76d>\u001b[0m in \u001b[0;36mread_multiple\u001b[0;34m(algs, exp)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-36b365ade76d>\u001b[0m in \u001b[0;36mread\u001b[0;34m(alg, exp)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"results/{algorithms[alg]['alg_id']}-{experiments[exp]['exp_id']}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mrepetitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repetition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'results/progressive_index-0.2-1024-zoom_in-70000000-1000-8-0.01.csv' does not exist: b'results/progressive_index-0.2-1024-zoom_in-70000000-1000-8-0.01.csv'"
     ]
    }
   ],
   "source": [
    "exps = ['Uniform8', 'Skewed8', 'ZoomIn8', 'Periodic8', 'SequentialZoomIn8', 'AlternatingZoomIn8', 'Shifting8', 'Sequential2',]\n",
    "m = metrics(exps, [\n",
    "        'MedianKDTree',\n",
    "        'AverageKDTree',\n",
    "        'Quasii',\n",
    "        'CrackingKDTree',\n",
    "        'ProgressiveIndex',\n",
    "        'ProgressiveIndexCostModel',\n",
    "        'FullScan',\n",
    "])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m.loc[idx[:, 'First Query'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=True))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payoff Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m[['AvgKD', 'MedKD', 'AKD', 'Q', 'PKD(.2)', 'GPKD(.2)']].loc[idx[:, 'PayOff'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format=\"%.0f\"))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query('Uniform2', ['Quasii',\n",
    "        'CrackingKDTree',\n",
    "        'ProgressiveIndex',\n",
    "        'ProgressiveIndexCostModel'], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m[['AKD', 'Q', 'PKD(.2)', 'GPKD(.2)']].loc[idx[:, 'Robustness'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format='%.E'))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m.loc[idx[:, 'Time'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format=\"%.1f\"))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_down('Periodic2', ['Quasii', 'CrackingKDTree'], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with different number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = []\n",
    "\n",
    "for i in [2, 4, 8, 16]:\n",
    "    exps.append(f'Uniform{i}')\n",
    "\n",
    "a = metrics(exps, [\n",
    "        'MedianKDTree',\n",
    "        'AverageKDTree',\n",
    "        'Quasii',\n",
    "        'CrackingKDTree',\n",
    "        'ProgressiveIndex',\n",
    "        'ProgressiveIndexCostModel',\n",
    "        'FullScan',\n",
    "])\n",
    "latex = pandas_to_latex(a, 'min', ignore_last=True)\n",
    "for exp in exps:\n",
    "    latex = latex.replace(experiments[exp]['name'], \"\\\\rotatebox[origin=c]{90}{%s}\" % experiments[exp]['name'])\n",
    "print(latex)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
