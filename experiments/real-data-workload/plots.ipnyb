{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import json\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config\n",
    "with open('config.json') as json_file:\n",
    "    f = json.load(json_file)\n",
    "    NUMBER_OF_QUERIES = f['number_of_queries']\n",
    "    REPETITIONS = f['repetitions']\n",
    "    ROWS = f['rows']\n",
    "    PARTITION_SIZE = f['partition_size']\n",
    "    PROGRESSIVE_INDEX_DELTAS = f['deltas']\n",
    "\n",
    "# Algorithms and Experiments defitions\n",
    "algorithms = {\n",
    "    'AverageKDTree': {\n",
    "        'name': 'average_kd_tree',\n",
    "        'color': 'red',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'AvgKD',\n",
    "        'type': 'full_index',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'MedianKDTree': {\n",
    "        'name': 'median_kd_tree',\n",
    "        'color': 'red',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'MedKD',\n",
    "        'type': 'full_index',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'CrackingKDTree': {\n",
    "        'name': 'cracking_kd_tree',\n",
    "        'color': 'green',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'AKD',\n",
    "        'type': 'adaptive',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'Quasii': {\n",
    "        'name': 'quasii',\n",
    "        'color': 'green',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'Q',\n",
    "        'type': 'adaptive',\n",
    "        'delta': '0.0',\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'FullScan': {\n",
    "        'name': 'full_scan_cl',\n",
    "        'color': 'black',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'FS',\n",
    "        'type': 'full_index',\n",
    "        'partition_size': '0',\n",
    "        'delta':'0.0',\n",
    "    },\n",
    "    'ProgressiveIndexCostModel': {\n",
    "        'name': 'progressive_index_cm',\n",
    "        'color': 'purple',\n",
    "        'dash': 'dot',\n",
    "        'show_name': 'GPKD(.2)',\n",
    "        'type': 'adaptive',\n",
    "        'delta': PROGRESSIVE_INDEX_DELTAS[0],\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    },\n",
    "    'ProgressiveIndex': {\n",
    "        'name': 'progressive_index',\n",
    "        'color': 'purple',\n",
    "        'dash': 'solid',\n",
    "        'show_name': 'PKD(.2)',\n",
    "        'type': 'adaptive',\n",
    "        'delta': PROGRESSIVE_INDEX_DELTAS[0],\n",
    "        'partition_size': PARTITION_SIZE\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, value in algorithms.items():\n",
    "    name = value['name']\n",
    "    delta = value['delta']\n",
    "    partition_size = value['partition_size']\n",
    "    algorithms[key]['alg_id'] = f\"{name}-{delta}-{partition_size}\"\n",
    "    \n",
    "# Real World Experiments\n",
    "experiments = {\n",
    "    'Power': {\n",
    "        'name': 'Power',\n",
    "        'name-in-file': 'power',\n",
    "        'sel': '0.0',\n",
    "        'n_rows': ROWS,\n",
    "        'n_queries': NUMBER_OF_QUERIES,\n",
    "        'base_folder': 'results'\n",
    "    },\n",
    "    'Skyserver': {\n",
    "        'name': 'Skyserver',\n",
    "        'name-in-file': 'skyserver',\n",
    "        'n_rows': '0',\n",
    "        'n_queries': '0',\n",
    "        'sel': '0.0',\n",
    "        'base_folder': 'results'\n",
    "    },\n",
    "    'Genomics Mixed': {\n",
    "        'name': f'Genomics',\n",
    "        'name-in-file': 'genomics_query_8',\n",
    "        'n_rows': ROWS,\n",
    "        'n_queries': '100',\n",
    "        'sel': '0.0',\n",
    "        'base_folder': 'results'\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, value in experiments.items():\n",
    "    name = value['name-in-file']\n",
    "    rows = value['n_rows']\n",
    "    n_queries = value['n_queries']\n",
    "    sel = value['sel']\n",
    "    experiments[key]['exp_id'] = f\"{name}-{rows}-{n_queries}-{sel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output\n",
    "def read(alg, exp):\n",
    "    if exp.startswith('Shifting'):\n",
    "        n_queries_per_run = 10\n",
    "        uni = read(alg, 'Uniform' + experiments[exp]['n_cols'])\n",
    "        n_runs = int(len(uni)/n_queries_per_run) - 1\n",
    "        df_final = uni.head(n_queries_per_run)\n",
    "        for _ in range(int(n_runs)):\n",
    "            temp = uni.head(n_queries_per_run).copy()\n",
    "            df_final = df_final.append(temp, ignore_index=True)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"results/{algorithms[alg]['alg_id']}-{experiments[exp]['exp_id']}.csv\")\n",
    "        repetitions = df['repetition'].max() + 1\n",
    "        step = int(len(df.index)/repetitions)\n",
    "        df_final = df[:step].copy().reset_index()\n",
    "        for rep in range(1, repetitions):\n",
    "            df_final += df[step * (rep) : step * (rep + 1)].copy().reset_index()\n",
    "\n",
    "        df_final = df_final/repetitions\n",
    "    \n",
    "    if 'index_search_time' not in df_final:\n",
    "        df_final['index_search_time'] = 0.0\n",
    "    if 'tuples_scanned' not in df_final:\n",
    "        df_final['tuples_scanned'] = 0.0\n",
    "    if 'number_of_nodes' not in df_final:\n",
    "        df_final['number_of_nodes'] = 0.0\n",
    "    df_final['query_time'] = df_final['initialization_time'] + df_final['index_search_time'] + df_final['scan_time'] + df_final['adaptation_time']\n",
    "    df_final['query_time_cumsum'] = df_final['query_time'].cumsum()\n",
    "    return df_final\n",
    "\n",
    "                     \n",
    "def read_multiple(algs, exp):\n",
    "    ''' Reads multiple algorithms in an experiment, return three arrays: dfs, colors, names\n",
    "    '''\n",
    "    dfs = []\n",
    "    colors = []\n",
    "    names = []\n",
    "    dashes = []\n",
    "    for alg in algs:\n",
    "        dfs.append(read(alg, exp))\n",
    "        names.append(algorithms[alg]['show_name'])\n",
    "        colors.append(algorithms[alg]['color'])\n",
    "        dashes.append(algorithms[alg]['dash'])\n",
    "    \n",
    "    return dfs, colors, dashes, names,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def get_first_query(df):\n",
    "    return df['query_time'].iloc[0]\n",
    "\n",
    "def get_payoff(df, baseline):\n",
    "    p = [i for i, x in enumerate(df['query_time_cumsum'] - (baseline['query_time_cumsum'])) if x > 0]\n",
    "    if len(p) == 0:\n",
    "        return len(df)\n",
    "    return p[-1]\n",
    "\n",
    "def get_convergence(df, df_type=''):\n",
    "    if df_type == 'full_index':\n",
    "        return 0\n",
    "    c = [i for i, x in enumerate(df['adaptation_time']) if x != 0.0]\n",
    "    if(len(c) == 0):\n",
    "        return len(c)\n",
    "    else:\n",
    "        return c[-1]\n",
    "\n",
    "def get_robustness(df, df_type=''):\n",
    "    if df_type == 'full_index':\n",
    "        return 0\n",
    "    return np.var(df['query_time'][:min(50, get_convergence(df, df_type))])\n",
    "\n",
    "def get_total_time(df, lower=0, upper=-1):\n",
    "    return df['query_time'][lower:upper].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures\n",
    "def create_figure(data=[]):\n",
    "    fig = go.Figure(\n",
    "        data=data,\n",
    "        layout=go.Layout(\n",
    "#             width=1500,\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            font=dict(\n",
    "                size=42\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                showline=True,\n",
    "                linewidth=2,\n",
    "                linecolor='black',\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor='lightgrey',\n",
    "                zeroline=False,\n",
    "                zerolinecolor='rgba(0, 0, 0, 0)',\n",
    "                ticks=\"inside\",\n",
    "                ticklen=5\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                showline=True,\n",
    "                linewidth=2,\n",
    "                linecolor='black',\n",
    "                ticks='inside',\n",
    "                zeroline=True,\n",
    "                ticklen=5\n",
    "            ),\n",
    "            legend=dict(\n",
    "                font=dict(\n",
    "                    size=30,\n",
    "                    color=\"black\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def line(exp, algs, attr, limit=2000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df[attr][:limit])\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "#                 marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=4, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(showlegend=True, yaxis_title=attr)\n",
    "    return fig\n",
    "\n",
    "def workload_selectivity(exp):\n",
    "    fig = create_figure()\n",
    "    df = read('FullScan', exp)\n",
    "    sel = ((df['tuples_scanned']/df['scan_overhead'])/df['tuples_scanned']) * 100\n",
    "    \n",
    "    fig.add_traces(\n",
    "        data=go.Scatter(\n",
    "            name='selectivity',\n",
    "            x=list(range(len(sel))),\n",
    "            y=sel,\n",
    "            mode='lines',\n",
    "            line=dict(width=4)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Selectivity (%)')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def per_query(exp, algs, limit=1000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        alg = algs[i]\n",
    "        per_query_times = np.array(df['query_time'][:limit]) * 1000\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Time (milliseconds)')\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_yaxes(type=\"log\")\n",
    "    return fig\n",
    "\n",
    "def cummulative(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        alg = algs[i]\n",
    "        per_query_times = np.array(df['query_time_cumsum'][:limit])\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Time (Seconds)')\n",
    "    return fig\n",
    "\n",
    "def number_of_nodes(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    biggest = 0\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df['number_of_nodes'][:limit])\n",
    "        if biggest < np.max(per_query_times):\n",
    "            biggest = np.max(per_query_times)\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='# Nodes')\n",
    "    return fig\n",
    "\n",
    "def tuples_scanned(exp, algs, limit=5000):\n",
    "    dfs, colors, dashes, names = read_multiple(algs, exp)\n",
    "    fig = create_figure()\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        per_query_times = np.array(df['tuples_scanned'][:limit])\n",
    "        lines.append(\n",
    "            go.Scatter(\n",
    "                name=names[i],\n",
    "                x=list(range(len(per_query_times))),\n",
    "                y=per_query_times,\n",
    "                marker_color=colors[i],\n",
    "                mode='lines',\n",
    "                line=dict(width=6, dash=dashes[i])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.add_traces(data=lines)\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(showlegend=True, yaxis_title='Tuples Scanned')\n",
    "    return fig\n",
    "\n",
    "def break_down(exp, algs, limit):\n",
    "    dfs, _, _, names = read_multiple(algs, exp)\n",
    "    initializations = np.array([x['initialization_time'][:limit].sum() for x in dfs])\n",
    "    adaptation = np.array([x['adaptation_time'][:limit].sum() for x in dfs])\n",
    "    search = np.array([x['index_search_time'][:limit].sum() for x in dfs])\n",
    "    scan = np.array([x['scan_time'][:limit].sum() for x in dfs])\n",
    "    \n",
    "#     fig = create_figure(data=[\n",
    "#         go.Bar(name='Initialization', x=names, y=initializations, marker_color='rgb(176, 201, 146)'),\n",
    "#         go.Bar(name='Adaptation', x=names, y=adaptation, marker_color='rgb(195, 114, 28)'),\n",
    "#         go.Bar(name='Index Search', x=names, y=search, marker_color='rgb(197, 255, 114)'),\n",
    "#         go.Bar(name='Scan', x=names, y=scan, marker_color='rgb(237, 218, 123)'),\n",
    "#     ])\n",
    "\n",
    "    scan_color = '#AB63FA'\n",
    "    index_search_color = '#03CC96'\n",
    "    adaptation_color = '#EF553B'\n",
    "    initialization_color = '#636EFA'\n",
    "\n",
    "    data = [\n",
    "        go.Bar(name='Initialization', x=names, y=initializations, width=0.5, marker_color=initialization_color, showlegend=False),\n",
    "        go.Bar(name='Adaptation', x=names, y=adaptation, width=0.5, marker_color=adaptation_color, showlegend=False),\n",
    "        go.Bar(name='Index Search', x=names, y=search, width=0.5, marker_color=index_search_color,showlegend=False),\n",
    "        go.Bar(name='Scan', x=names, y=scan, width=0.5, marker_color=scan_color,showlegend=False),\n",
    "    ]\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Initialization',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=initialization_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Adaptation',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=adaptation_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Index Search',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=index_search_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    data.append(\n",
    "        go.Scatter(\n",
    "            name='Scan',\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker_color=scan_color,\n",
    "            marker=dict(\n",
    "                size=40,\n",
    "                symbol='square'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = create_figure(data=data)\n",
    "    \n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(legend_orientation=\"h\", legend=dict(x=.25, y=1.2))\n",
    "    fig.update_layout(yaxis_title='Time (seconds)')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex Tables\n",
    "\n",
    "def pandas_to_latex(df, highlight='min', ignore_last=False, float_format=\"%.2f\"):\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        c_min = float('inf')\n",
    "        c_max = -float('inf')\n",
    "        if ignore_last:\n",
    "            length = len(row) - 1\n",
    "        else:\n",
    "            length = len(row)\n",
    "        for j in range(length):\n",
    "            if row[j] == '-' or row[j] == '*':\n",
    "                continue\n",
    "            if c_min > float(row[j]):\n",
    "                c_min = float(row[j])\n",
    "            if c_max < float(row[j]):\n",
    "                c_max = float(row[j])\n",
    "        mins.append(c_min)\n",
    "        maxs.append(c_max)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        for j in range(len(row)):\n",
    "            if row[j] == '-' or row[j] == '*':\n",
    "                continue\n",
    "            if highlight == 'min':\n",
    "                if float(row[j]) == mins[i]:\n",
    "                    row[j] = \"\\cellcolor{green!25}\" + (float_format % float(row[j]))\n",
    "                else:\n",
    "                    row[j] = float_format % float(row[j])\n",
    "            if highlight == 'max':\n",
    "                if float(row[j]) == maxs[i]:\n",
    "                    row[j] = \"\\cellcolor{green!25}\" + (float_format % float(row[j]))\n",
    "                else:\n",
    "                    row[j] = float_format % float(row[j])\n",
    "    \n",
    "    return df.to_latex(multicolumn=True, multirow=True, escape=False)\n",
    "\n",
    "def metrics(exps, algs):\n",
    "    '''\n",
    "    ||||||||||||Algorithm 1| Algorithm 2|...\n",
    "    First Query|   1.11.   |     15.    |...\n",
    "    ...\n",
    "    '''\n",
    "    data = {}\n",
    "\n",
    "    # create indexes\n",
    "    index_exp = []\n",
    "    index_metric = []\n",
    "    \n",
    "    \n",
    "    metrics = ['First Query', 'PayOff', 'Convergence', 'Robustness', 'Time']\n",
    "\n",
    "    \n",
    "    for exp in exps:\n",
    "        dfs, _, _, names = read_multiple(algs, exp)\n",
    "        \n",
    "        # initialize the data dict with empty arrays for each algorithm\n",
    "        for name in names:\n",
    "            if name not in data:\n",
    "                data[name] = []\n",
    "\n",
    "        baseline = read('FullScan', exp)\n",
    "\n",
    "        index_exp += [experiments[exp]['name']] * len(metrics)\n",
    "        index_metric += metrics\n",
    "        \n",
    "        for df, name, alg in zip(dfs, names, algs):\n",
    "            data[name].append('%.2f' % get_first_query(df))\n",
    "            po = get_payoff(df, baseline)\n",
    "            if po == len(df):\n",
    "                data[name].append('-')\n",
    "            else:\n",
    "                data[name].append(po)\n",
    "            conv = get_convergence(df, algorithms[alg]['type'])\n",
    "            if conv == 0:\n",
    "                data[name].append('-')\n",
    "            elif conv >= len(df)-1:\n",
    "                data[name].append('*')\n",
    "            else:\n",
    "                data[name].append(conv)\n",
    "            \n",
    "            robust = get_robustness(df, algorithms[alg]['type'])\n",
    "            if robust == 0:\n",
    "                data[name].append('-') \n",
    "            else:\n",
    "                data[name].append('%.E' % robust)\n",
    "            \n",
    "            \n",
    "            data[name].append('%.2f' %get_total_time(df))\n",
    "\n",
    "    index = [index_exp, index_metric]\n",
    "    df = pd.DataFrame(data, index=index)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    latex = df.to_latex(multicolumn=True, multirow=True)\n",
    "\n",
    "    for exp in exps:\n",
    "        latex = latex.replace(exp, \"\\\\rotatebox[origin=c]{90}{%s}\" % exp)\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ['Power', 'Genomics Mixed', 'Skyserver']\n",
    "m = metrics(exps, [\n",
    "        'MedianKDTree',\n",
    "        'AverageKDTree',\n",
    "        'Quasii',\n",
    "        'CrackingKDTree',\n",
    "        'ProgressiveIndex',\n",
    "        'ProgressiveIndexCostModel',\n",
    "        'FullScan',\n",
    "])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m.loc[idx[:, 'First Query'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=True))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payoff Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m[['AvgKD', 'MedKD', 'AKD', 'Q', 'PKD(.2)', 'GPKD(.2)']].loc[idx[:, 'PayOff'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format=\"%.0f\"))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m[['AKD', 'Q', 'PKD(.2)', 'GPKD(.2)']].loc[idx[:, 'Robustness'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format='%.E'))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "a = m.loc[idx[:, 'Time'], :]\n",
    "a = a.reset_index(level=1, drop=True)\n",
    "print(pandas_to_latex(a, 'min', ignore_last=False, float_format=\"%.1f\"))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
